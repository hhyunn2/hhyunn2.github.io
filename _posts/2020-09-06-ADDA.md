---
title: "ADDA 논문 리뷰"
category :
    - Domain Adaptation
header:
  teaser: "/assets/images/artificial-intelligence.jpg"

tag :
    - Domain Adaptation
toc : true
---


## 1. Introduction

Dataset bias와 domain shift로 인하여 하나의 데이터셋으로 학습한 모델이 다른 데이터셋이 들어왔을때 생각만큼의 효과를 얻을 수 없는 경우가 많습니다. 이를 해결할 수 있는 방법으로 task-specific한 데이터셋을 통한 fine-tune이 있을 수 있겠지만 어렵고 비용이 많이 드는 작업입니다.  따라서 이번 논문의 토픽이라고 할 수 있는 domain adaptation(이하 DA)이 도입되었습니다. Domain shift의 영향을 최소화하기 위해 두 도메인을 common feature space에 맵핑하도록 DA가 적용되었다고 합니다. 

Adversarial adaptation 방법은 도메인 차이의 distace를 domain discriminator에 대한 adversarial objective를 통해 최소화하도록 하는 방법입니다.

저자는 discriminative representation이 궁극적인 목표이기에 generative 모델링이 크게 필요하지 않다고 설명합니다. 따라서 본 논문의 ADDA 모델은 discriminative representation을 source labels를 통해 먼저 학습을 진행하고 별도의 인코딩이 domain adversarial loss로 인하여 학습되는 과정으로 진행되게 됩니다. 



---

## 2. Related Work

관련 내용들을 간단하게 정리하면

- Maximum Mean Discrepancy(MMD) loss를 활용하는 방법
  
  - 두 도메인의 평균 사이의 norm 구하는 방법입니다.
  
- Adversarial loss를 사용해서 domain shift를 줄이는 방법
  - Source의 label은 구분하는 동시에 domains는 구분하지 못하도록 하는 방법입니다. 
  
- GAN

  - Generator와 Discriminator를 활용하여 G의 경우 loss가 줄어들도록, D의 경우 loss가 증가하도록 학습이 되어 최종적으로 구분실제와 비슷하도록 이미지가 생성되는 모델입니다.  
  - 자세한 것은 본 논문의 링크를 참고 바랍니다. <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets" target="_blank"> GAN논문 </a>     

- BiGAN
  
  - inverse mapping을 학습
  
  - 자세한 것은 본 논문의 링크를 참고 바랍니다. <a href="https://arxiv.org/abs/1605.09782" target="_blank"> BiGAN논문 </a>
  
- CGAN
  
  - GAN의 확장된 모델로서 추가적인 정보를 담은 vector가 input으로 들어가게 됩니다.
  - 자세한 것은 본 논문의 링크를 참고 바랍니다.  <a href="https://arxiv.org/abs/1411.1784" target="_blank"> CGAN논문 </a>
  
- CoGAN
  
  - 두 개의 GAN을 각각 source와 target 이미지들로 학습시켜 domain transfer를 진행시키는 모델입니다.  
  - 자세한 것은 본 논문의 링크를 참고 바랍니다.   <a href="https://arxiv.org/abs/1606.07536" target="_blank"> CoGAN논문 </a>

다음과 같습니다. 관련 내용들은 링크를 첨부하오니 관심이 있으시다면 해당 논문들을 참고바랍니다. 본 논문은 위의 관련 내용들의 흐름에 따라 만들어진 모델이라고 할 수 있겠습니다. 이전의 CoGAN과의 차이점을 본 논문에서는 이미지 분포에 대한 모델링이 크게 필요하지 않다는 것이라고 소개하고 있으며 discriminative approach를 통해 실현하였습니다.



---

## 3. Generalized Adversarial Adaptation

- Source and target

  - Source image $X_s$, Source labels $Y_s$, Source domain distribution $p_s(x,y)$, Source mapping $M_s$, Source classifier $C_s$
- Target image $X_t$, No target labels, Target domain distribution $p_t(x,y)$, Target mapping $M_t$, Target classifier $C_t$

Adversarial adaptive 방법의 목표는 source와 target의 mapping, 즉 $M_s$와 $M_t$를 정형화하면서 $M_s\left(X_s\right)$와 $M_t\left(X_t\right)$의 거리를 좁혀주는 것입니다. 그리고 이를 만족하게 된다면 classifier의 경우에는 $C_s$가 그대로 target representations에 사용되어도 괜찮아질 것입니다.  즉, $C_t$를 따로 학습할 필요없이 $C=C_t=C_s$를 만족하게 됩니다.

Domain discriminator(이하 D)는 들어온 값이 source의 도메인을 가지는지 혹은 target의 도메인을 가지는지 판별해주는 역할을 합니다. D는 standard supervised loss에 의해서 최적화됨을 알 수 있습니다.

수식첨부

$M_s$와 $M_t$의 최적화의 경우 제한된 adversarial objective에 의해 진행됩니다. 

수식첨부





```markdown
<figure>
	<img src="{{ '/assets/img/object_detection_first/fig2_classification_example.PNG' | prepend: site.baseurl }}" alt=""> 
	<figcaption> [그림 2. Image Classification 예시] </figcaption>
</figure> 
```


$$
\min_{M_s, C} \mathcal{L}_{cls}(X_s, Y_s) = \\
-\mathbb E_{(x_s,y_s)~(X_s,Y_s)} \sum^K_{k=1}
$$

#### 3-1. Source and target mapping

우선적으로 mapping parameterization을 할 필요가 있습니다. 

이전의 모델들의 경우에는 discriminative 모델들을 두 도메인에 적응시키려는 노력이 있었습니다. 

Mapping parameterization이 이루어지면 어떻게 $M_t$를 parametrize할지 결정합니다.



#### 3-2. Adversarial losses

$M_t$의 parametrization을 결정하게 된다면 adversarial loss를 도입하여 실제 맵핑을 학습하게 됩니다. 이때 loss는 여러가지가 활용될 수 있는데 



---

## 4. Adversarial Discriminative Domain Adaptation

모델을 구성하기 위해서 세 가지 중요한 선택사항이 있다고 합니다.  

- Generative와 Discriminative 중 어떠한 것을 베이스로 선택할 것인가
- Weights를 공유할 것인가
- 어떤 loss를 사용할 것인가

이 중 본 논문에서는 'Disriminative를 바탕으로 하는 모델, Unshared weights, standard GAN loss' 이 세 가지를 선택하였습니다.

이 선택의 배경으로 저자는 이렇게 설명하고 있습니다.

- Generate에 사용되는 많은 파라미터들이 discriminative adaptation 과정과 무관한 경우가 많다고 가정을 하고 진행하였다고 합니다. 
- ㅇ
- ㅇ

ㅇㄹㅇㄹ



---

## 5. Experiments

위의 모델의 결과를 확인하기 위하여 본 논문은 MNIST, SVHN, USPS 데이터를 활용하는 실험과  NYUD 데이터를 활용하는 실험으로 DA를 진행하였습니다. 

#### 5-1. MNIST, USPS and SVHN digit datasets

사진첨부

표첨부

#### 5-2. Modality adaptation

사진첨부

표첨부